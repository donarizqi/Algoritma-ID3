---
title: "Dona Rizqi Aulia"
date: "`r Sys.Date()`"
author: Dona Rizqi Aulia-Statistika dan Bisnis Muhammadiyah
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
bibliography: references.bib
---

```{=html}
<style>


body{
text-align: justify}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pengertian
Algoritma Iterative Dichotomiser 3 (ID3) ditemukan oleh J. Ross
Quinlan th 1986 digunakan untuk menghasilkan pohon keputusan
dari suatu dataset.

ID3 (Iterative Dichotomiser 3) merupakan salah satu metode dalam
data mining yang arsitekturnya dengan pohon keputusan dibuat
menyerupai bentuk pohon.

Dimana pada umumnya sebuah pohon terdapat akar (root), cabang
yang hanya terdapat satu input dan min dua output dan daun (leaf
node).

Pembentukan pohon klasifikasi dengan ID3 ada 2 Langkah, yaitu
menghitung nilai entropy dan menghitung nilai information gain dari
setiap variable.

# Thapan Algoritma ID3
1. Menyiapkan dataset
2. Menghitung Nilai Entropy
3. Menghitung nilai Gain
4. Membuat node cabang dari nilai gain yang terbesar
5. Proses perhitungan Information Gain terus dilakukan sampai semua    data masuk kedalam kelas yang sama. Atribut yang telah dipilih tidak diikutkan lagi dalam nilai Information Gain.

# Eksperimen
```{r}
library(dplyr)
```

## Masukan Dataset
```{r}
library(readxl)
Data_penguin <- read_excel("~/ALGORITMA-ID3/Data penguin.xlsx")
View(Data_penguin)
```

Nilai Entropy pada setiap fitur/kolom dapat dihitung sebagai berikut:
```{r}
IsPure <- function(data) {
  length(unique(data[,ncol(data)])) == 1
}
Entropy <- function( vls ) {
  res <- vls/sum(vls) * log2(vls/sum(vls))
  res[vls == 0] <- 0
  -sum(res)
}
entropy <- function(edible) Entropy(c(edible, 100 - edible))
entropy <- Vectorize(entropy)
curve( entropy, from = 0, to = 100, xname = 'edible')
```

Kemudian, berikut adalah bagaimana menghitung nilai Information Gain dari setiao fitur/kolom dalam suatu dataset

```{r}
library(rpart)
library(rpart.plot)
trees<-rpart(Species ~., data = iris, method = 'class')
rpart.plot(trees)
```

```{r}
InformationGain <- function( tble ) {
  tble <- as.data.frame.matrix(tble)
  entropyBefore <- Entropy(colSums(tble))
  s <- rowSums(tble)
  entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
  informationGain <- entropyBefore - entropyAfter
  return (informationGain)
}
```

```{r}
InformationGain(table(Data_penguin[,c('Culmen.Length', 'Species')]))
```

# Referensi

<https://rpubs.com/philjet/shannonentropy>
<https://victorzhou.com/blog/information-gain/>
<https://informatikalogi.com/algoritma-id3/>
